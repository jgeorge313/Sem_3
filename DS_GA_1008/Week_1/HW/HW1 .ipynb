{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991672c1",
   "metadata": {},
   "source": [
    "# Joby George\n",
    "# HW 1\n",
    "# DS GA 1008\n",
    "## Due 9/29/22\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac932d",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "\n",
    "## 1.1\n",
    "\n",
    "You are given the following neural net architecture:\n",
    "\n",
    "$$Linear_1 \\rightarrow \\mathcal{f} \\rightarrow Linear_2 \\rightarrow g  $$\n",
    "\n",
    "where $Linear_i(x)$ = $\\bold{W^{(i)}x + b^{(i)}}$ is the i-th affine transformation, and *f,g* are element-wise nonlinear activation functions. when an input x $\\in$ $R^{n}$ is fed to the network, $\\hat{y}$ $\\in$ $R^{k}$ is obtaied as the output\n",
    "\n",
    "## 1.2\n",
    "\n",
    "We would like to perform the regression task. we choose f(.) = 5ReLU(.) and g to be the identity function. To train the network, we chose MSE loss function:\n",
    "\n",
    "$$ l_{MSE}(\\hat{y},y) = ||\\hat{y} - y||^2$$\n",
    "\n",
    "where y is the target output. \n",
    "\n",
    "### Questions\n",
    "\n",
    "Is y a real number or a vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cf47b",
   "metadata": {},
   "source": [
    "### A\n",
    "\n",
    "Name and mathematically describe the 5 programming steps you would take to train this model with PyTorch using SGD on a single batch of data.\n",
    "\n",
    "\n",
    "### Answer: \n",
    "\n",
    "        1: \n",
    "        2:\n",
    "        3:\n",
    "        4:\n",
    "        5:\n",
    "\n",
    "# Look this up using the lab notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaaf88c",
   "metadata": {},
   "source": [
    "### B\n",
    "\n",
    "For a single datapoint(x,y),write down all inputs and outputs for forward pass of each layer. You can only use variable $\\bold{x,y, W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}}$ in your answer. (note that  $Linear_i(x)$ = $\\bold{W^{(i)}x + b^{(i)}}$) \n",
    "\n",
    "### Answer\n",
    "The overall set of steps can be defined procedeurally as: \n",
    "\n",
    "$$ Input\\space layer: x \\in R^{n} \\rightarrow Linear_1 \\rightarrow \\mathcal{f} \\rightarrow Linear_2 \\rightarrow g$$\n",
    "\n",
    "$$z_1 = Linear_1 = W^{(1)}x +b^{(1)}$$\n",
    "\n",
    "$$z_2 = f(z_1) = 5ReLu(z_1)$$\n",
    "\n",
    "$$z_3 = Linear_2(z_2) = W^{(2)}z_2 +b^{(2)}$$\n",
    "\n",
    "$$\\hat{y} = g(z_3) = I_kz_3$$\n",
    "\n",
    "$$L(\\hat{y},y) = ||\\hat{y}-y||^2$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "Are we also allowed to use $\\hat{y}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417f741",
   "metadata": {},
   "source": [
    "### C\n",
    "\n",
    "Write down the gradients calculated from the backward pass. You can only use the following variables:$\\bold{x,y, W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}},\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}}, \\frac{\\partial{z_2}}{\\partial{z_1}}, \\frac{\\partial{\\hat{y}}}{\\partial{z_3}}$ in your answer.\n",
    "\n",
    "### Answer:\n",
    "The derivatives in the backpropogation are the change in our loss function with respect to our inputs, and our weights, represented by: $\\frac{\\partial{l}}{x}$, $\\frac{\\partial{l}}{W^{(k)}}$ and $\\frac{\\partial{l}}{b^{(k)}}$\n",
    "\n",
    "$$\\frac{\\partial{l}}{x} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{x_i}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{W_2} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{W_2}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{b_2} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{b_2}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{W_1} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{W_1}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{b_1} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{b_1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec509e5",
   "metadata": {},
   "source": [
    "### D\n",
    "\n",
    "Show us the elements of:\n",
    "\n",
    "$$1.D.A: \\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}}$$\n",
    "$$1.D.B: \\frac{\\partial{\\hat{y}}}{\\partial{z_3}}$$\n",
    "$$1.D.C: \\frac{\\partial{z_3}}{\\partial{z_2}}$$\n",
    "    \n",
    "(be careful about the dimensionality).\n",
    "\n",
    "### 1.D.A\n",
    "\n",
    "$$L(\\hat{y},y) = ||\\hat{y}-y||^2$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = \n",
    "<\\hat{y}-y>^T<\\hat{y}-y>\\frac{d}{\\partial{\\bf{{\\hat{y}}}}}$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = \\hat{y}^2 - 2\\hat{y}y + y^2 \\frac{d}{\\partial{\\bf{{\\hat{y}}}}}$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = 2(\\hat{y}-y)$$\n",
    "\n",
    "This partial has a dimension of $R^k$ (the number of classes)\n",
    "### 1.D.B\n",
    "$$\\hat{y} = I_kz_3$$\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = I_kz_3\\frac{d}{\\partial{z_3}}$$\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = I_k$$\n",
    "\n",
    "This partial has a dimension of $R^{kxk}$.\n",
    "### 1.D.C\n",
    "$$z_3 = W^{(2)}z_2 +b^{(2)}$$\n",
    "$$\\frac{\\partial{z_3}}{\\partial{z_2}} = W^{(2)}z_2 +b^{(2)}\\frac{d}{\\partial{z_2}}$$\n",
    "$$\\frac{\\partial{z_3}}{\\partial{z_2}} = W^{(2)T}$$\n",
    "\n",
    "This partial has a dimension of $R^{(kxd)}$ where d is the number of columns in $W_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6592e6",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "\n",
    "We would like to perform multi-class classification task, so we set f = tanh and g = $\\sigma$ the logistic sigmoid function:\n",
    "\n",
    "$$\\sigma(z) = (1+exp(-x))^{-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79366da",
   "metadata": {},
   "source": [
    "### A\n",
    "\n",
    "If you want to train this network, what do you need to change in the equations of:\n",
    "\n",
    "    1.2.B\n",
    "    1.2.C \n",
    "    1.2.D  \n",
    "\n",
    "assuming we are using the same MSE loss function\n",
    "\n",
    "### Answer for 1.2.B\n",
    "\n",
    "While the network's computational diagram remains the same for the feedforward functions, the activation functions have changed giving us:\n",
    "\n",
    "$$ Input\\space layer: x \\in R^{n} \\rightarrow Linear_1 \\rightarrow \\mathcal{f} \\rightarrow Linear_2 \\rightarrow g$$\n",
    "\n",
    "$$z_1 = Linear_1 = W^{(1)}x +b^{(1)}$$\n",
    "\n",
    "$$z_2 = f(z_1) = tanh(z_1)$$\n",
    "\n",
    "$$z_3 = Linear_2(z_2) = W^{(2)}z_2 +b^{(2)}$$\n",
    "\n",
    "$$\\hat{y} = g(z_3) = \\sigma(z_3)$$\n",
    "\n",
    "$$L(\\hat{y},y) = ||\\hat{y}-y||^2$$\n",
    "\n",
    "### Answer for 1.2.C\n",
    "\n",
    "The backpropogatiaon algorithm finds how changes in our inputs and parameters impacts our loss function. Because the computational graph is still the same the partial derivatives calculated during the backpropagation algorithm remain the same, namely: \n",
    "\n",
    "$$\\frac{\\partial{l}}{x} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{x}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{W_2} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{W_2}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{b_2} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{b_2}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{W_1} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{W_1}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{b_1} = \\frac{\\partial{l}}{\\partial{\\hat{y}}}*\n",
    "\\frac{\\partial{\\hat{y}}}{z_3}*\n",
    "\\frac{\\partial{z_3}}{\\partial{z_2}}*\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}}*\n",
    "\\frac{\\partial{z_1}}{\\partial{b_1}}\n",
    "$$\n",
    "\n",
    "### Answer for 1.2.D\n",
    "\n",
    "Since we have changed the activation functions, we have some new gradients, however, because the loss function and linear layers remain the same $\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}}$ and \n",
    "$\\frac{\\partial{z_3}}{\\partial{z_2}}$ are the same as 1.2.D.\n",
    "\n",
    "$$L(\\hat{y},y) = ||\\hat{y}-y||^2$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = \n",
    "<\\hat{y}-y>^T<\\hat{y}-y>\\frac{d}{\\partial{\\bf{{\\hat{y}}}}}$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = \\hat{y}^2 - 2\\hat{y}y + y^2 \\frac{d}{\\partial{\\bf{{\\hat{y}}}}}$$\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\bf{{\\hat{y}}}}} = 2(\\hat{y}-y)$$\n",
    "\n",
    "$$z_3 = W^{(2)}z_2 +b^{(2)}$$\n",
    "$$\\frac{\\partial{z_3}}{\\partial{z_2}} = W^{(2)}z_2 +b^{(2)}\\frac{d}{\\partial{z_2}}$$\n",
    "$$\\frac{\\partial{z_3}}{\\partial{z_2}} = W^{(2)T}$$\n",
    "\n",
    "$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}}$ changes, and the new partial is claculated below: \n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = \\sigma(z_3)\\frac{d}{\\partial{z_3}}$$\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = (1+e^{-z_3})^{-2}*\\frac{d}{\\partial{z_3}}(1+e^{-z_3})$$\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = \\frac{e^{-z_3}}{(1+e^{-z_3})^{-2}}$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = \\sigma(z_3)(1-\\sigma(z_3))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b32b02",
   "metadata": {},
   "source": [
    "### B \n",
    "\n",
    "Now you think you can do a better job by using a Binary Cross Entropy (BCE) loss function \n",
    "\n",
    "$$l_{BCE}(\\hat{y},y) = \\frac{1}{K}\\sum_{i=1}^{K}(-y_ilog(\\hat{y_i}))+[(1-y_i)log(1-\\hat{y_i})]$$\n",
    "\n",
    "What do you need to change in the equations of (b), (c) and (d)\n",
    "\n",
    "### Answer 1.2.B\n",
    "\n",
    "We would change the last layer to take into account the new loss function\n",
    "\n",
    "$$ Input\\space layer: x \\in R^{n} \\rightarrow Linear_1 \\rightarrow \\mathcal{f} \\rightarrow Linear_2 \\rightarrow g$$\n",
    "\n",
    "$$z_1 = Linear_1 = W^{(1)}x +b^{(1)}$$\n",
    "\n",
    "$$z_2 = f(z_1) = tanh(z_1)$$\n",
    "\n",
    "$$z_3 = Linear_2(z_2) = W^{(2)}z_2 +b^{(2)}$$\n",
    "\n",
    "$$\\hat{y} = g(z_3) = \\sigma(z_3)$$\n",
    "\n",
    "$$L(\\hat{y},y) =\\frac{1}{K}\\sum_{i=1}^{K}(-y_ilog(\\hat{y_i}))+[(1-y_i)log(1-\\hat{y_i})]$$\n",
    "\n",
    "### Answer 1.2.C\n",
    "\n",
    "The overall backpropogation algorithim computes the same partials, with respect to the same variables as previously. The difference is that $\\frac{\\partial{l}}{\\partial{\\hat{y}}}$ will now have changed, which will be shown in the next part.\n",
    "\n",
    "### Answer 1.2.D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaae258",
   "metadata": {},
   "source": [
    "### C\n",
    "\n",
    "Things are getting better. You realize that not all intermediate hidden activations need to be binary (or soft version of binary). You decide to use f (·) = (·)+ but keep g as $\\sigma{z_3}$. Explain why this choice of f can be beneficial for training a (deeper) network.\n",
    "\n",
    "\n",
    "### Answer\n",
    "\n",
    "The choice of ReLu as an activation function in comparison with the tanh function will help training a deeper network because the gradient will persist through the backpropogation algorithm. Since the derivative of ReLu(x) is:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\nabla ReLu(z) =\n",
    "   \\begin{cases}\n",
    "        1 & \\text{if } z > 0\\\\\n",
    "        0 & \\text{if } z \\leq 0\n",
    "    \\end{cases}\n",
    "\\end{equation}$\n",
    "\n",
    "In comparison: \n",
    "$\\begin{equation}\n",
    "   \\nabla tanh(z) =\n",
    "   \\begin{cases}\n",
    "        0+ & \\text{if } z \\leq -5 \\space \\text{or } z \\geq 5\\\\\n",
    "       (0,1] & \\text{if } z \\in [-5,5]\n",
    "    \\end{cases}\n",
    "\\end{equation}$\n",
    "\n",
    "Thus our gradient will be close to 0 if our input to the activation function takes a large positive or negative value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22fadf",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "\n",
    "### A\n",
    "Why is softmax actually softargmax?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "Goodfellow, Bengio and Courville explain this succintly in [page 183 of Deep Learning](https://www.deeplearningbook.org/contents/mlp.html).\n",
    "\n",
    "The function is more similar to the arg max function than the max function.\n",
    "The term “soft” derives from the fact that the softmax function is continuous and differentiable as the arg max function is represented as a one-hot vector, and is not continuous nor differentiable. The softmax function thus provides a “softened” version of the arg max. \n",
    "\n",
    "To get the more literal definition of softmax, we would perform:\n",
    "\n",
    "$$softmax(z) = softargmax(z)^Tz$$\n",
    "\n",
    "This would give us a differentiable way of calculating the maximum value of our input array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40586b0",
   "metadata": {},
   "source": [
    "### B\n",
    "Draw the computational graph defined by this function, with inputs $x,y,z \\in \\mathcal{R}$ and output $w \\in \\mathcal{R}$\n",
    "\n",
    "You make use symbols x,y,z,o, and operators ∗,+ in your solution. Be sure to use the correct shape for symbols and operators as shown in class.\n",
    "\n",
    "$$ a = x * y + z$$\n",
    "$$ b= (x+x)*a$$\n",
    "$$ w= a*b$$\n",
    "\n",
    "### Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d12e1",
   "metadata": {},
   "source": [
    "### C\n",
    "\n",
    "Draw the graph of the derivative for the following functions:\n",
    "\n",
    "    ReLU()\n",
    "    LeakyReLU(negative_slope=.01)\n",
    "    Softplus(beta=1)\n",
    "    GELU()\n",
    "\n",
    "### Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acf577",
   "metadata": {},
   "source": [
    "### d\n",
    "\n",
    "What are 4 different types of linear transformations? What are the roles of linear transfomration and non linear transformation in a neural network.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "Linear Transformations can be:\n",
    "            \n",
    "            1. Rotation (orthogonal matrix w/determinant one)\n",
    "            2. Reflection (matrix w/negative determinant)\n",
    "            3. Scaling (multiply by a diagonal matrix)\n",
    "            4. Shearing (multiply by a non-diagonal matrix)\n",
    "            5. Projection (mapping a dimmension to 0)\n",
    "    \n",
    "Linear transformations followed by non linear transformations serve to take our inputs and \"rotate and squash\" them. Linear transformations often map our input to a higher dimmensional reprsentation where this hidden state is then \"squashed\" by our nonlinear functions (ReLU, tan(h), etc).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c58f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8669883",
   "metadata": {},
   "source": [
    "### Maybe useful derivatives:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next deriative is the change in $\\hat{y}$ with respect to $z_3$. \n",
    "\n",
    "$$\\hat{y} = I_kz_3$$\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = I_kz_3$$\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{z_3}} = I_k$$\n",
    "\n",
    "The next deriative is the change in $z_3$ with respect to $z_2$. \n",
    "\n",
    "$$z_3 = W^{(2)}z_2 +b^{(2)}$$\n",
    "$$\\frac{\\partial{z_3}}{\\partial{z_2}} = W^{(2)T}$$\n",
    "\n",
    "The next deriative is the change in $z_2$ with respect to $z_1$. \n",
    "\n",
    "$$z_2 = 5ReLu(z_1)$$\n",
    "$\\begin{equation}\n",
    "\\frac{\\partial{z_2}}{\\partial{z_1}} =\n",
    "   \\begin{cases}\n",
    "        5 & \\text{if } z_1 > 0\\\\\n",
    "        0 & \\text{if } z_1 \\leq 0\n",
    "    \\end{cases}\n",
    "\\end{equation}$\n",
    "\n",
    "The next deriative is the change in $z_1$ with respect to $x_i$. \n",
    "\n",
    "$$\\frac{\\partial{z_1}}{\\partial{x}} = W^{(1)T}$$\n",
    "\n",
    "If we look at the additional gradients needed to update our parameter $W^{(2)}$:\n",
    "$$\\frac{\\partial{z_3}}{\\partial{W_2}} = z_2 $$ \n",
    "\n",
    "If we look at additional the gradients needed to update our parameter $b^{(2)}$:\n",
    "$$\\frac{\\partial{z_3}}{\\partial{b_2}} = 1 \\space \\in \\space  R^{k} $$ \n",
    "\n",
    "If we look at the additional gradients needed to update our parameter $W^{(1)}$:\n",
    "$$\\frac{\\partial{z_1}}{\\partial{W_1}} = z_1 $$ \n",
    "\n",
    "If we look at additional the gradients needed to update our parameter $b^{(2)}$:\n",
    "$$\\frac{\\partial{z_1}}{\\partial{b_1}} = 1 \\space \\in \\space R^{D}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaaf88c",
   "metadata": {},
   "source": [
    "### Maybe useful: B\n",
    "\n",
    "For a single datapoint(x,y),write down all inputs and outputs for forward pass of each layer. You can only use variable $\\bold{x,y, W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}}$ in your answer. (note that  $Linear_i(x)$ = $\\bold{W^{(i)}x + b^{(i)}}$) \n",
    "\n",
    "### Answer\n",
    "The overall set of steps can be defined procedeurally as: \n",
    "\n",
    "$$ Input\\space layer: x \\in R^{n} \\rightarrow Linear_1 \\rightarrow \\mathcal{f} \\rightarrow Linear_2 \\rightarrow g$$\n",
    "\n",
    "$$z_1 = Linear_1 = W^{(1)}x +b^{(1)}$$\n",
    "\n",
    "$$z_2 = f = 5ReLu(z_1)$$\n",
    "\n",
    "$$z_3 = Linear_2 = W^{(2)}z_2 +b^{(2)}$$\n",
    "\n",
    "$$\\hat{y} = g = I_kz_3$$\n",
    "\n",
    "$$L(\\hat{y},y) = ||\\hat{y}-y||^2$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "Are we also allowed to use $\\hat{y}$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
