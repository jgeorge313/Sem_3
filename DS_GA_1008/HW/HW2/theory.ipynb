{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS GA 1008 HW 2\n",
    "# Joby George\n",
    "# Due 10/13 4 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Theory\n",
    "\n",
    "# Section 1.1 Convultional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1.A\n",
    "\n",
    "Given an input image of dimmension 21 x 12 what will the output dimmension be after applying a convolution with a 4x5 kernel, stride of 4 and no padding?\n",
    "\n",
    "## 1.1.A Answer\n",
    "\n",
    "To help in my understanding of this problem, I've attached a visual diagram that shows how this convolution would impact the input image.\n",
    "\n",
    "![convolution](stride.png)\n",
    "\n",
    "We can see that there are two differentiations of colors, light vs dark, indicating the column output dimmension of this convolution would be 2.\n",
    "\n",
    "There are 5 primary types of colors, indicating the row dimmension would be 5. Thus our output would be a **cx5x2** where c is the number of channels for this input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 B\n",
    "Given an input of dimmension C X H X W what will be the dimension of the output of a convolutional layer with kernel of size K x K, padding P, stride S, dilation D, and F filters. Assume that H $\\geq K,  W \\geq K$\n",
    "\n",
    "## 1.1.B Answer\n",
    "\n",
    "To help me understand visually how stride and dilation worked together, i created a visualization.\n",
    "\n",
    "![dilation.png](dilation.png)\n",
    "\n",
    "It became apparent after applying a convolution of size K X K, with padding 0, stride S, Dilation D, and filters (or num convolutions) F we get:\n",
    "\n",
    "$$ouput \\in R^{(F \\text{x} C \\text{x} \\lfloor (H-DK+0+S)/S \\rfloor \\text{x} \\lfloor (W-DK+0+S)/S \\rfloor)}$$\n",
    "\n",
    "where $\\lfloor x \\rfloor$ denotes the floor function on x\n",
    "\n",
    "Therefore, after applying a **convolution of size K X K, with padding P, stride S, Dilation D, and filters (or num convolutions) F** we get:\n",
    "\n",
    "$$ouput \\in R^{(F \\text{x} C \\text{x} \\lfloor (H-DK+P+S)/S \\rfloor \\text{x} \\lfloor (W-DK+P+S)/S \\rfloor)}$$\n",
    "\n",
    "### QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.C\n",
    "\n",
    "In this section we are going to work with 1-dimensional convolutions. Discrete convolution of 1-dimensional input x[n] and kernel k[n] is defined as follows:\n",
    "\n",
    "$$ s[n] = (x*k)[n] = \\sum_m x[n-m]k[m] $$\n",
    "\n",
    "However, in machine learning convolution is usually implemented as cross-correlation which is defined as follows:\n",
    "\n",
    "$$ s[n] = (x*k)[n] = \\sum_m x[n+m]k[m] $$\n",
    "\n",
    "Note the difference in signs, which will get the network to learn a \"flipped\" kernel. In general it doesn't change much, but it's important to keep it in mind. In convolution neural networks, the kernel k[n] is usually 0 everywhere, except a few values near 0: $\\forall_{|n|>M}k[n] = 0$. Then the formula becomes \n",
    "\n",
    "$$ s[n] = (x*k)[n] = \\sum_{m=-M}^M x[n+m]k[m] $$\n",
    "\n",
    "Let's consider an input x[n] $\\in R^5$, with 1 $\\leq n \\leq$ 7 e.g. it is a length 7 sequence with 5 channels. We consider the convolutional layer f_{W} with one filter, with kernel size 3, stride of 2, no dilation and no padding. The only parameters of the convolutional layer is the weight W, w $\\in R^{1x5x3}$, there's no bias and no non-linearity\n",
    "\n",
    "### 1.1.C.I\n",
    "\n",
    "What is the dimension of the output $f_{W}(x)$? Provide an expression for the value of elements of the convolutional layer output $f_{W}(x)$. Example answer format here and in the following problems \n",
    "\n",
    "$$f_{W}(x) \\in R^{42X42X42}, f_{W}(x)[i,j,k] = 42$$\n",
    "\n",
    "\n",
    "### 1.1.C.I Answer\n",
    "\n",
    "$$f_{W}(x) \\in R^{1x5X3}$$\n",
    "\n",
    "Going forward, let $\\hat{y}$ = $f_{W}(x)$.  \n",
    "\n",
    "Thinking about the first convolution, we take a 3 unit interval of the first channel's input to get a vector $\\in R^{1x3}$. We take the dot product with $W_1$ a vector $\\in R^{3x1}$ and perform the dot product giving us a scalar value. \n",
    "\n",
    "We can represent all 5 channels by taking the first 3 units of all 5 channels, giving us a data matrix $\\in R^{5X3}$ and perform the dot product of each channel with the corresponding weight, matrix each with dimmension $\\in R^{3x1}$ giving us a resulting vector $\\in R^{5x1}$\n",
    "\n",
    "Because there is a stride of 2, we then take inputs x[3],x[4],x[5] and repeat this process giving us another $R^{5x1}$, and lastly we repeat this operation one last time using inputx x[5],x[6],x[7] to get our third vecotr $\\in R^{5x1}$.\n",
    "\n",
    "$$\\hat{y}_{1,j,k} = \\sum_{m=1}^3W_{j+m} \\cdot X_{j,2k-1+m} $$\n",
    "\n",
    "While the notation is dense, essentially we are taking the dot product between the $R^{1x3}$ weight matrix for channel j and element wise multiplying it by the associated inputs, for a given i,j in our output matrix.\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\\hat{y}_{1,2,3} = W_{21}*x_{25}+W_{22}*x_{26}+W_{23}*x_{27}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.C.II\n",
    "\n",
    "What is the dimension of $\\frac{\\partial{\\hat{y}}}{\\partial{W}}$?  Provide an expression for $\\frac{\\partial{\\hat{y}}}{\\partial{W}}$\n",
    "\n",
    "### 1.1.C.II Answer\n",
    "$\\frac{\\partial{\\hat{y}}}{\\partial{W}} \\in R^{5x3x3}$\n",
    "\n",
    "Thinking about our convolution operator on one channel, \n",
    "\n",
    "$$ \\hat{y}_{(1,1)} = W_{11}*x_{11}+W_{12}x_{12}+W_{13}x_{13}$$\n",
    "$$ \\hat{y}_{(1,2)} = W_{11}*x_{13}+W_{12}x_{14}+W_{13}x_{15}$$\n",
    "$$ \\hat{y}+{(1,3)} = W_{11}*x_{15}+W_{12}x_{16}+W_{13}x_{17}$$\n",
    "\n",
    "If we were to take the $\\frac{\\partial{\\hat{y}_1}}{\\partial{W_1}}$ we get:\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}}_1}{\\partial{W_1}} = \\begin{bmatrix}\n",
    "    \\frac{\\partial{\\hat{y}}_{11}}{\\partial{W_{11}}} &  \\frac{\\partial{\\hat{y}}_{11}}{\\partial{W_{12}}} &  \\frac{\\partial{\\hat{y}}_{11}}{\\partial{W_{13}}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{\\hat{y}}_{12}}{\\partial{W_{11}}} &  \\frac{\\partial{\\hat{y}}_{12}}{\\partial{W_{12}}} &  \\frac{\\partial{\\hat{y}}_{12}}{\\partial{W_{13}}}\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{\\hat{y}}_{13}}{\\partial{W_{11}}} &  \\frac{\\partial{\\hat{y}}_{13}}{\\partial{W_{13}}} &  \\frac{\\partial{\\hat{y}}_{13}}{\\partial{W_{13}}}\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Re-expressing this: \n",
    "$$\\frac{\\partial{f\\hat{y}_1}}{\\partial{W_1}} = \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} \\\\\n",
    "    x_{13} & x_{14} & x_{15} \\\\\n",
    "    x_{15} & x_{16} & x_{17}\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Expanding this to the general form:\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{W}_{ijk}} = X_{1,i,2j+k-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.C.III\n",
    "\n",
    "What is the dimension of $\\frac{\\partial{f_{W}(x)}}{\\partial{x}}$?  Provide an expression for $\\frac{\\partial{f_{W}(x)}}{\\partial{x}}$\n",
    "\n",
    "### 1.1.C.III Answer\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{x}} \\in R^{5x3x7}$$\n",
    "\n",
    "Recalling the work from the previous problem\n",
    "\n",
    "$$ \\hat{y}_{(1,1)} = W_{11}*x_{11}+W_{12}x_{12}+W_{13}x_{13}$$\n",
    "$$ \\hat{y}_{(1,2)} = W_{11}*x_{13}+W_{12}x_{14}+W_{13}x_{15}$$\n",
    "$$ \\hat{y}_{(1,3)} = W_{11}*x_{15}+W_{12}x_{16}+W_{13}x_{17}$$\n",
    "\n",
    "If we were to take the $\\frac{\\partial{f_{W}(x)_1}}{\\partial{W_1}}$ we get:\n",
    "\n",
    "\n",
    " $$\\frac{\\partial{\\hat{y}}_1}{\\partial{x_1}} = \\begin{bmatrix}\n",
    "    \\frac{\\partial{\\hat{y}}_{11}}{\\partial{x_{11}}} &  \\frac{\\partial{\\hat{y}}_{11}}{\\partial{x_{12}}} & ... & \\frac{\\partial{\\hat{y}}_{11}}{\\partial{x_{17}}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{\\hat{y}}_{12}}{\\partial{x_{11}}} &  \\frac{\\partial{\\hat{y}}_{12}}{\\partial{x_{12}}} & ... & \\frac{\\partial{\\hat{y}}_{12}}{\\partial{x_{17}}}\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{\\hat{y}}_{13}}{\\partial{x_{11}}} &  \\frac{\\partial{\\hat{y}}_{12}}{\\partial{x_{12}}} & ... & \\frac{\\partial{\\hat{y}}_{13}}{\\partial{x_{17}}}\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Re-expressing this:\n",
    "\n",
    "$$\\frac{\\partial{\\hat{y}}_1}{\\partial{x_1}} = \\begin{bmatrix}\n",
    "    W_{11} &  W_{12} & W_{13} & 0 & 0 & 0 & 0 \\\\\n",
    "    \\\\\n",
    "    0 &  0 & W_{11} & W_{12} & W_{13} & 0 & 0 \\\\\n",
    "    \\\\\n",
    "    0 &  0 & 0 & 0 & W_{11} & W_{12} & W_{13} \n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Expanding this to the general form:\n",
    "$$\\frac{\\partial{\\hat{y}}}{\\partial{x}_{ijk}} = \n",
    "  \\begin{cases}\n",
    "        W_{1,\\lfloor k/j \\rfloor} \\text{    if} \\space \\frac{k}{j} < 2 \\\\\n",
    "        W_{1,\\lceil k/j \\rceil} \\text{     if} \\space 2 \\leq \\frac{k}{j} \\leq 3 \\\\\n",
    "        0 \\space \\text{else}\n",
    "    \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.C.IV\n",
    "\n",
    "Now suppose you are given the gradient of the loss $\\mathcal{l}$ w.r.t the output of the convolutional layer $f_{W}(x)$, i.e. $\\frac{\\partial{\\mathcal{l}}}{\\partial{f_{W}(x)}}$ Whta is the dimension of $\\frac{\\partial{\\mathcal{l}}}{\\partial{W}}$? Provide an expression for $\\frac{\\partial{\\mathcal{l}}}{\\partial{W}}$. Explain similarities and differences of this expression and expression in (i).\n",
    "\n",
    "\n",
    "### 1.1.C.IV Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Recurrent Neural Networks\n",
    "\n",
    "## 1.2.1\n",
    "\n",
    "In this section we consider a simple recurrent neural network defined as follows:\n",
    "\n",
    "$$c[t] = \\sigma(W_cx[t]+W_hh[t-1])$$\n",
    "$$h[t] = c[t] \\odot h[t-1] + (1-c[t]) \\odot W_xx[t]$$\n",
    "\n",
    "Where $\\sigma$ is elmeent-wise sigmoid, $x[t] \\in R^n, h[t] \\in R^m, W_c \\in R^{mxn}, W_h \\in R^{mxm}, \\odot$ is the Hadamard product (element wise multiplicaiton), h[0] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1.A\n",
    "\n",
    "Draw a diagaram for this recurrent neural network, similar to the diagarm of rNN we had in class. We suggest using diagrams.net\n",
    "\n",
    "## 1.2.1.A Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1.B\n",
    "\n",
    "What is the dimension of c[t]\n",
    "\n",
    "\n",
    "## 1.2.1.B Answer\n",
    "\n",
    "The dimmension of c[t] is $R^{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1.C\n",
    "\n",
    "Suppose that we run the RNN to get a sequence of h[t] for t from 1 to K. Assuming we know the derivative $\\frac{\\partial{\\mathcal{l}}}{\\partial{h[t]}}$, provide the dimmensions of, and an expression for values of $\\frac{\\partial{\\mathcal{l}}}{\\partial{W_x}}$, What are the similarities of backward pass and forward pass in this RNN?\n",
    "\n",
    "\n",
    "## 1.2.1.C Answer\n",
    "\n",
    "Writing out the gradient, using the chain rule, we see:\n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{l}}}{\\partial{W_x}} = \\frac{\\partial{\\mathcal{l}}}{\\partial{h[t]}}\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W}}$$\n",
    "\n",
    "Since we can assume the first partial is known, we focus in on $\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W_x}}$. Writing out the expression for h[t] we see:\n",
    "\n",
    "$$h[t] = c[t] \\odot h[t-1] + (1-c[t]) \\odot W_xx[t]$$\n",
    "\n",
    "Since we are taking the derivative of a h[t] $\\in R^m$ with a matrix $W_x \\in R^{mxn}$, $\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W_x}}$ must be of dimmension $R^{mxmxn}$.\n",
    "\n",
    "Looking at the first object of this $R^{mxmxn}$ tensor, we see:\n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W_x}}_{1jk} = \\begin{bmatrix}\n",
    "    \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x11}}} & \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x12}}} & ... &  \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x1n}}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x21}}} & \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x22}}} & ... & \\frac{\\partial{h[t]_{1}}}{\\partial{W_{x2n}}} \\\\\n",
    "    \\\\\n",
    "    ... & ... & ... & .... \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{h[t]_{1}}}{\\partial{W_{xm1}}} & \\frac{\\partial{h[t]_{1}}}{\\partial{W_{xm2}}} & ... &  \\frac{\\partial{h[t]_{1}}}{\\partial{W_{xmn}}}\n",
    "\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "We know that the first index of h[t] is the Hadmarand product of 1-c[t] and the first element of $W_x$x.\n",
    "\n",
    "Therefore, if we were to change the first row of $W_x$ we would see resulting changes in the first index of h[t]. However, changes to values outside of the first row of $W_x$ will not change the first index of h[t], giving us: \n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W_x}}_{1jk} = \\begin{bmatrix}\n",
    "    (1-c[t])_1*x[t]_{1} & (1-c[t])_1*x[t]_{2} & ... & (1-c[t])_1*x[t]_{n} \\\\\n",
    "    \\\\\n",
    "    0 & 0 & 0 & 0 \\\\\n",
    "    ... & ... & ... & .... \\\\\n",
    "    \\\\\n",
    "    0 & 0 & 0 & 0\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Generalizing this, we see: \n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{W_x}}_{ijk} = \\begin{cases}\n",
    "        (1-c[t])_i*x[k] \\text{    if} \\space j = i \\\\\n",
    "        0 \\space \\text{else}\n",
    "    \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1.D\n",
    "\n",
    "Can this network be subject to vanishing or exploding gradients? Why?\n",
    "\n",
    "## Answer\n",
    "\n",
    "The network can be subject to vanishing gradients as a result of the multiplication by (1-c[t]). Since the sigmoid function has a range of (0,1) and pushes higher values closer to 0 and smaller values closer to 0, the result of this operation will likely result in a multiplication by a small number when c[t] takes a large value. \n",
    "\n",
    "The problem compounds itself by the recurrent infrastructure. When we unravelling the derivative, the previous hidden state is used as an input, thus the number of times we perform this multiplication is a function of T. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2\n",
    "\n",
    "We define an AttentionRNN(2) as:\n",
    "\n",
    "$$q_0[t],q_1[t],q_2[t] = Q_0x[t], Q_1h[t-1], Q_2h[t-2]$$\n",
    "$$k_0[t],k_1[t],k_2[t] = K_0x[t], K_1h[t-1], K_2h[t-2]$$\n",
    "$$v_0[t],v_1[t],v_2[t] = V_0x[t], V_1h[t-1], V_2h[t-2]$$\n",
    "\n",
    "\n",
    "$$w_i[t] = q_i[t]^Tk_i[t]$$\n",
    "$$a[t] = softargmax([w_0[t],w_1[t],w_2[t]))$$\n",
    "$$h[t] = \\sum_{i=0}^2a_i[t]v_i[t]$$\n",
    "\n",
    "Where $x_i[t], h[t] \\in R^n$, and $Q_i, K_i, V_i \\in R^{nxn}$. We define h[t] = 0 for t <1. You may safely ignore these base cases in the following questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1.2.2.A\n",
    "\n",
    "Draw a diagram for this recurrent neural network\n",
    "\n",
    "## 1.2.2.A Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2.B\n",
    "\n",
    "What is the dimmension of a[t]?\n",
    "\n",
    "### 1.2.2.B Answer\n",
    "\n",
    "a[t] is $\\in R^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2.C\n",
    "\n",
    "Extend this to, AttentionRNN(k), a network that uses the last k state vectors h. Write out the system of equations that defines it. You may use set notation or elipses in your definintion.\n",
    "\n",
    "### 1.2.2.C Answer\n",
    "\n",
    "k $\\in {1....k}, k \\leq T$ \n",
    "\n",
    "$$q_0[t],q_1[t],q_2[t], ... q_k[k] = Q_0x[t], Q_1h[t-1], Q_2h[t-2], ... Q_kh[t-k]$$\n",
    "$$k_0[t],k_1[t],k_2[t],... k_k[t] = K_0x[t], K_1h[t-1], K_2h[t-2],... K_kh[t-k]$$\n",
    "$$v_0[t],v_1[t],v_2[t]... v_k[t] = V_0x[t], V_1h[t-1], V_2h[t-2],... V_kh[t-k]$$\n",
    "\n",
    "\n",
    "$$w_i[t] = q_i[t]^Tk_i[t]$$\n",
    "$$a[t] = softargmax([w_0[t],w_1[t],w_2[t], ... w_k[t]])$$\n",
    "$$h[t] = \\sum_{i=0}^ka_i[t]v_i[t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2.D\n",
    "\n",
    "Modify the above network to produce AttentionRNN($\\infty$), a network that uses every past state vector. Write out the system of equations that defines it. You may use set notation or elipses in your definintion. **HINT**: We can do this by tying together some set of parameters, e.g. weight sharing.\n",
    "\n",
    "## 1.2.2.D Answer\n",
    "\n",
    "k $\\in {1....T} $ \n",
    "\n",
    "$$q_0[t],q_1[t],q_2[t], ... q_k[k] = Q_0x[t], Q_1h[t-1], Q_2h[t-2], ... Q_kh[t-k]$$\n",
    "$$k_0[t],k_1[t],k_2[t],... k_k[t] = K_0x[t], K_1h[t-1], K_2h[t-2],... K_kh[t-k]$$\n",
    "$$v_0[t],v_1[t],v_2[t]... v_k[t] = V_0x[t], V_1h[t-1], V_2h[t-2],... V_kh[t-k]$$\n",
    "\n",
    "\n",
    "$$w_i[t] = q_i[t]^Tk_i[t]$$\n",
    "$$a[t] = softargmax([w_0[t],w_1[t],w_2[t], ... w_k[t]])$$\n",
    "$$h[t] = \\sum_{i=0}^ka_i[t]v_i[t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2.E\n",
    "\n",
    "Suppose the loss l is computed, and we know the derivative $\\frac{\\partial{\\mathcal{l}}}{\\partial{h[t]}}$. Please write down the expression for $\\frac{\\partial{h[t]}}{\\partial{h[t-1]}}$ for AttentionRNN(2)\n",
    "\n",
    "## 1.2.2.E Answer\n",
    "\n",
    "Since we are taking a derivative of a vector $\\in R^n$ w.r.t a vector $\\in R^n$ our Jacobian will be an $R^{nxn}$ matrix, looking something like this:\n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{h[t-1]}_{jk}} = \\begin{bmatrix}\n",
    "\n",
    "\n",
    "    \\frac{\\partial{h[t]_{1}}}{\\partial{h[t-1]_{1}}} & \\frac{\\partial{h[t]_{1}}}{\\partial{h[t-1]_{2}}} & ... &  \\frac{\\partial{h[t]_{1}}}{\\partial{h[t-1]_{n}}} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{h[t]_{2}}}{\\partial{h[t-1]_{1}}} & \\frac{\\partial{h[t]_{2}}}{\\partial{h[t-1]_{2}}} & ... & \\frac{\\partial{h[t]_{2}}}{\\partial{h[t-1]_{n}}}\\\\\n",
    "    \\\\\n",
    "    ... & ... & ... & .... \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial{h[t]_{n}}}{\\partial{h[t-1]_{1}}} & \\frac{\\partial{h[t]_{n}}}{\\partial{h[t-1]_{2}}} & ... &  \\frac{\\partial{h[t]_{n}}}{\\partial{h[t-1]_{n}}}\n",
    "\n",
    "\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "To help understand this calculus, let's create a derived experiment. Let $Q_1, K_1, V_1$ equal the $Id_{3}$ matrix. Let h[t-1], $v_0[t]$ and $v_2[t]$ be vectors $\\vec{\\bold{1}} \\in R^{3}$ and let $w_0, w_2$ = 3\n",
    "\n",
    "$$q_1[t], k_1[t],v_1[t]= \\vec{\\bold{1}}$$\n",
    "$$w_1[t] = 3$$\n",
    "$$\\alpha_0, \\alpha_1, \\alpha_2 = .33$$\n",
    "$$h[t] = \\alpha_0 *v_{0,1} + \\alpha_1 * v_{1,1} + \\alpha_2 *v_{2,1}$$\n",
    "$$h[t] = .33 *\\vec{\\bold{1}} + .33 * \\vec{\\bold{1}} + .33 *\\vec{\\bold{1}}$$ \n",
    "$$h[t] = \\vec{\\bold{1}}$$ \n",
    "\n",
    "Now, imagine we change the first element of h[t-1] to a 2, i.e:\n",
    "\n",
    "$$h[t-1] = \\begin{bmatrix}\n",
    "            2 \\\\\n",
    "            1 \\\\\n",
    "            1 \n",
    "            \\end{bmatrix}$$\n",
    "\n",
    "$$q_1[t], k_1[t],v_1[t]= h[t-1]$$\n",
    "$w_1[t] = 6$$\n",
    "\n",
    "\n",
    "$v_{0,1}$ denotes the $v_0$ vector's first index\n",
    "\n",
    "Looking at the above, the only terms that depends on h[t-1] are $\\alpha_1$  and $v_{1,1}$\n",
    "$$\\alpha_0, \\alpha_2 = .045$$\n",
    "$$\\alpha_1 = .91$$\n",
    "$$h[t] = .045 *\\vec{\\bold{1}} + .91 *v_1[t] + .045 *\\vec{\\bold{1}}$$ \n",
    "$$h[t] = \\begin{bmatrix}\n",
    "            1.91 \\\\\n",
    "            1 \\\\\n",
    "            1 \n",
    "            \\end{bmatrix}$$\n",
    "\n",
    "Therefore, we can **clearly** observe that when we change h[t-1], we change every $\\alpha$ term, impacting h[t].\n",
    "\n",
    "Expressing this in more generalizable terms: \n",
    "\n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{h[t]}}}{\\partial{h[t-1]}} = \\frac{partial{h[t]}}{\\frac{partial{\\alpha_0}}} + {\\frac{partial{\\alpha_1}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2.2.F\n",
    "\n",
    "Suppose we know the derivative $\\frac{\\partial{h[t]}}{\\partial{h[T]}}$ for all t > T. Please write down the expression for $\\frac{\\partial{\\mathcal{l}}}{\\partial{h[T]}}$ for AttentionRNN(k) \n",
    "\n",
    "## 1.2.2.F Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 \n",
    "\n",
    "What caused the spikes on the left?\n",
    "\n",
    "## 1.3.1 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 \n",
    "\n",
    "How can they be higher than the initial value of the loss?\n",
    "\n",
    "## 1.3.2 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3\n",
    "\n",
    "What are some ways to fix them? \n",
    "\n",
    "## 1.3.3 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.4\n",
    "\n",
    "Explain why the loss and accuracy are at these values before training starts. You mayn eed to check the task definition in the notebook.\n",
    "## 1.3.4 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the dimension of the output $f_{W}(x)$? Provide an expression for the value of elements of the convolutional layer output $f_{W}(x)$. Example answer format here and in the following problems \n",
    "\n",
    "$$f_{W}(x) \\in R^{42X42X42}, f_{W}(x)[i,j,k] = 42$$\n",
    "\n",
    "\n",
    "### 1.1.C.I Answer\n",
    "\n",
    "$$f_{W}(x) \\in R^{5X3}$$\n",
    "\n",
    "Thinking about the first convolution, we take a 3 unit interval of the first channel's input to get a vector $\\in R^{1x3}$. We take the dot product with $W_1$ a vector $\\in R^{3x1}$ and perform the dot product giving us a scalar value. \n",
    "\n",
    "We can represent all 5 channels by taking the first 3 units of all 5 channels, giving us a data matrix $\\in R^{5X3}$ and perform the dot product of each channel with the corresponding weight, matrix each with dimmension $\\in R^{3x1}$ giving us a resulting vector $\\in R^{5x1}$\n",
    "\n",
    "Because there is a stride of 2, we then take inputs x[3],x[4],x[5] and repeat this process giving us another $R^{5x1}$, and lastly we repeat this operation one last time using inputx x[5],x[6],x[7] to get our third vecotr $\\in R^{5x1}$.\n",
    "\n",
    "Visualizing this, using python indexing notation of a matrix X[i,j] with indicies ranging from 0 $\\leq i \\leq 7$ and 0 $\\leq j \\leq 5$\n",
    "\n",
    "$$f_{W}(x) = <x[2j:2j+3][i],W[i]>$$\n",
    "\n",
    "Re-express this to be a little nicer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
